{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第三章 线性回归方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 线性回归模型和最小二乘法\n",
    "1. **残差的平方和**    \n",
    "$RSS(\\beta) = (y-X\\beta)^T(y-X\\beta)$    &emsp;(1)    \n",
    "$\\frac{\\partial RSS}{\\partial \\beta} = -2X^T(y-X\\beta)$    &emsp;(2)     \n",
    "$\\frac{\\partial^2 RSS}{\\partial\\beta\\partial\\beta^T} = 2X^TX $     &emsp;(3)    \n",
    "由（2）式可得，当X列满秩时，    \n",
    "$X^T(y-X\\beta) = 0 $    &emsp;（4）    \n",
    "有唯一解：    \n",
    "$\\hat\\beta = (X^TX)^{-1}X^Ty$.   &emsp; (5)    \n",
    "所以 $\\hat y = X\\hat\\beta = X(X^TX)^{-1}X^Ty $&emsp;(6)    \n",
    "当X列满秩时，由（4）式中的正交性可得，下图中的$y-\\hat y$垂直于所有X构成的样本子空间。    \n",
    "当X不是列满秩时，不满足正交性，y在X的子空间的映射$\\hat y$可能有多种方式，解决的办法是去掉多余的列。   \n",
    "\n",
    "    <img src='img/orthgonal_y.png' width=320 height=320 align = \"center\"/>    \n",
    "2. **一些基本的数学分布** \n",
    "    * 指数分布：    \n",
    "    $f(x;\\beta) = \\begin{cases}\\frac{1}{\\beta} e^{-x/\\beta}, &x>0\\\\0, &其他\\end{cases}$    \n",
    "    指数分布的均值和方差分别为$\\beta$和$\\beta^2$    \n",
    "    * 伽马分布：    \n",
    "    来源于伽马函数： $\\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha-1}e^{-x}dx $。    \n",
    "    再对伽马分布进行定义：\n",
    "    连续随机变量X服从参数为\\alpha和\\beta的伽马分布，其密度函数满足下式：    \n",
    "    $f(x;\\alpha,\\beta) = \\begin{cases}\\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{\\alpha -1}e^{-x/\\beta},&x>0\\\\0,&\\text{其他}\\end{cases}$    \n",
    "    伽马分布的均值和方差分别为$\\alpha\\beta$和$\\alpha\\beta^2$, 当$\\alpha$为1时,伽马分布即为指数分布。\n",
    "    * 卡方分布：若n个相互独立的随机变量ξ₁、ξ₂、……、ξn ，均服从标准正态分布（也称独立同分布于标准正态分布），则这n个服从标准正态分布的随机变量的平方和$\\sum \\limits_{i=1}^{n}\\xi_{i}^{2} $构成一新的随机变量，其分布规律称为$\\chi^2$分布（chi-square distribution），其中参数n称为自由度。\n",
    "    * t分布: 小样本的分布，正态分布可以算作其特例，相对于正态分布，多了一个参数：自由度，v=n-1，自由度（样本数量）越大，t分布越集中，越接近正态分布，此外，t分布相对于正态分布重要的特性“温良宽厚”，即更能容忍偏离点的出现，允许两侧尾部出现的概率较高，这在小样本出现偏离点的情况下能保持更稳定的分布效果。    \n",
    "    $ T=\\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}$     \n",
    "    在涉及对总体均值进行推断的问题中，或在涉及对照样本（即判断两个样本的均值是否有显著差别）的问题中，t分布有大量应用。此外，t检验用t分布，Z检验用z分布，两者都是用于样本平均值差异性检验的方法，前者用于小样本（样本数小于30),后者用于大样本。    \n",
    "    * F分布: F统计两个独立卡方随机变量的比率，且每个随即便离那个都要除以其自由度：    \n",
    "    $ F = \\frac{U/v_1}{V/v_2}$    \n",
    "    式中，U和V分别是服从自由度为$v_1$和$v_2$的卡方分布的独立随机变量。    \n",
    "    F分布也称为**方差比率分布**，围绕两个来源的变差：    \n",
    "    （1） 样本内的变差    \n",
    "    （2） 样本间的变差    \n",
    " \n",
    "    参考：[t分布、卡方分布，F分布](https://www.cnblogs.com/think-and-do/p/6509239.html)    \n",
    "    &nbsp;\n",
    "3. $\\beta$的样本属性    \n",
    "假设观察值$y_i$相互无关且方差为常数$\\theta^2$:    \n",
    "&emsp; $\\hat \\theta^2 = \\frac{1}{N-p-1}\\sum \\limits_{i=1}^{N}(y_i-{\\hat y}_i)^2$    &emsp;(7)    \n",
    "那么：    &emsp; $Var(\\hat \\beta) = (X^TX)^{-1}\\theta^2 $     &emsp;(8)     \n",
    "显而易见：&emsp; $\\hat\\beta \\sim N(\\beta, (X^TX)^{-1}\\theta^2) $ &emsp; (9)    \n",
    "同时对于多个满足相同正态分布的$y_i$： &emsp; $(N-p-1)\\theta^2 \\sim \\theta^2\\chi^2_{N-p-1} $ &emsp;    \n",
    "满足自由度为N-p-1的卡方分布。接下来我们使用这些性质完成$\\beta_j$的假设检验和置信区间的确定。    \n",
    "&nbsp;\n",
    "4. 检验特定参数$\\beta_j = 0$,我们使用Z统计量：    \n",
    "&emsp; $z_j = \\frac{{\\hat\\beta}_j}{\\hat\\theta\\sqrt{v_j}}$&emsp;&emsp;(10).   \n",
    "其中$v_j$是$(X^TX)^{-1}$第j个对角元素。\n",
    "5. 检验是否可以去掉一个特定的变量，即$\\beta$中的一个参数，通过最终RSS的值检验效果，我们使用F统计量：    \n",
    "&emsp; $F=\\frac{(RSS_0 - RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)}$.&emsp;&emsp;(11)    \n",
    "6. 我们还可以单独获取$\\beta_j$的$1-2\\alpha$的置信区间:     \n",
    "$({\\hat \\beta}_j-z^{(1-\\alpha)}v^{\\frac{1}{2}}_j\\hat{\\theta}\\ ,\\quad {\\hat\\beta}_j+z^{(1-\\alpha)}v^{\\frac{1}{2}}_j\\hat{\\theta}\\ ) $   &emsp; (12)     \n",
    "7. 还可以用卡方统计量观察$\\hat\\beta$是否在对应的置信区间内:    \n",
    "&emsp; $C_\\beta = \\{ \\beta | (\\hat\\beta - \\beta)^TX^TX(\\hat\\beta-\\beta) \\le {\\hat\\theta}^2{\\chi_{p+1}^2}^{(1-\\alpha)}\\}$. &emsp; (13)\n",
    "\n",
    "    *可以用假设检验和置信区间的方式排除无用参数，但工程中一般不会考虑这种严谨繁琐的步骤吧*\n",
    "8. 高斯马尔科夫定理：    \n",
    "在经典的线性回归假设下，最小二乘估计量是方差最小的无偏估计量(注意是对参数估计的方差最小，而不是预测方差）。    \n",
    "&emsp; $MSE(\\hat\\theta) = E(\\hat\\theta - \\theta)^2$    \n",
    "&emsp; &emsp; &emsp; &emsp;$= Var(\\hat\\theta) + [E(\\hat\\theta) - E(\\theta)]^2$    \n",
    "可见，严格无偏的估计量是不必要的，有时我们使用有偏估计量去减少方差，例如岭回归和其他一些方法如正则化等。  \n",
    "9. 从单变量回归到多变量回归\n",
    "单变量回归中，引入内积：$<x,y> = x^Ty$后，可得：    \n",
    "&emsp; $\\hat\\beta = \\frac{<x,y>}{<x,x>},$.   \n",
    "&emsp; $\\gamma = y-x\\hat\\beta$.   \n",
    "上式是多变量回归的基础，给定输入$x_1,x_2,...,x_p$,假设相互正交，那么当$j \\ne k$时，$<x_j,x_k>=0$，所以$\\hat\\beta$等于$<x_j,y>/<x_j,x_j>$，当然真实观察的数据一般都不会正交，在此为了说明一些问题或者方便表示和解决，我们尝试将输入的列向量正交化。     \n",
    "一般的做法是使用格拉姆施密特正交法，如下算法：    \n",
    "\n",
    "    <img src='img/algorithm3_1.png' width=640 height=320 align = \"center\"/>   \n",
    "    \n",
    "    最终的结果：    \n",
    "    &emsp; $ \\hat \\beta_j = \\frac{<z_p,y>}{<z_p,z_p>}$&emsp; (14).    \n",
    "    \n",
    "    重新排列第二步中的$z_j$，能发现每个$x_j$都是$z_k$的线性组合，又因为$z_k$彼此正交，形成了X列空间的一组基。\n",
    "    此外，$\\hat\\beta_j$也代表着$x_j$对y的额外贡献，当$x_j$已经被$x_1,x_2,...,x_{j-1},x_{j+1},...,x_p$调整过后。假设$x_j$与$x_k$高度相关，那么$z_j$接近于0，那么通过（14）式能推出（15）式，$\\hat\\beta_j$会变得非常不稳定。    \n",
    "    &emsp; $ Var(\\hat\\beta_p) = \\frac{\\delta^2}{<z_p,z_p>} = \\frac{\\delta^2}{\\lVert z_p \\rVert^2} $&emsp;(15).        \n",
    "    我们也能将算法中的第二步表示为矩阵形式：\\ $X=Z\\Gamma$, \\ 其中Z由列向量$z_j$组成，$\\Gamma$是由所有$\\hat\\gamma_{kj}$组成的上三角矩阵，然后引入对角元素为$D_{jj} = \\lVert z_j \\rVert$的对角矩阵，得到：    \n",
    "    &emsp; $X\\ =\\ ZD^{-1}D\\Gamma$    \n",
    "    &emsp;&emsp;  $=\\ QR$,    \n",
    "    上述即为QR分解，Q是Nx(p+1)的标准正交矩阵，$Q^TQ = I$,R是(p+1)x(p+1)的上三角矩阵。QR分解为X的列空间提供了一组方便计算的基($R^{-1}$方便计算)，使得最小二乘法的结果为：    \n",
    "    &emsp; $ \\hat\\beta\\ =\\ R^{-1}Q^Ty,$.   \n",
    "    &emsp; $ \\hat y\\ =\\ QQ^Ty,$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 子集选择    \n",
    "最小二乘法有两个问题：    \n",
    "* 预测准确率：偏差虽然低，但是方差很大，通常会牺牲掉一些偏差而降低方差\n",
    "* 模型表示：相对于太多的参数，通常选择其中的子集，获得较好的全局效果，而舍弃细节。    \n",
    "下面列举一些子集选择的方法。  \n",
    "1. 最优子集选择法\n",
    "2. 前向（后向）逐步选择\n",
    "3. 前向阶段选择    \n",
    "通常会各种方法如前向后向结合。    \n",
    "\n",
    "## 3.4 Shrinkage Methods     \n",
    "子集的选择是一步步离散的，通常会有高方差，而shrinkage Method更加连续\n",
    "### 3.4.1 Ridge Regression\n",
    "$ {\\hat \\beta}^{ridge} = \\arg \\min \\limits_p \\{\\sum \\limits^N_{i=1}(y_i - \\beta_0 - \\sum \\limits^p_{j=1} x_{ij}\\beta_j)^2\\ +\\ \\lambda\\sum\\limits^p_{j=1}\\beta^2_j\\}$    \n",
    "也可以写为：    \n",
    "$ {\\hat \\beta}^{ridge} = \\arg \\min \\limits_p \\sum \\limits^N_{i=1}(y_i - \\beta_0 - \\sum \\limits^p_{j=1} x_{ij}\\beta_j)^2$,     \n",
    "&emsp;&emsp; $subject\\ to\\  \\sum\\limits_{j=1}^p \\beta^2_j \\le t, $    \n",
    "一般需要对输入标准化，才能保证参数在输入的比例内等变化变动。    \n",
    "**岭回归也可以从后验分布推出**，其中假定y服从后验分布：$y_i\\ \\sim\\ N(\\beta_0 + x^T_i\\beta,\\ \\delta^2)$,参数$\\beta$满足先验分布$N(0,\\tau^2)$。    \n",
    "由于一般过程中，X不是方阵，所以可以由矩阵的奇异值分解求$X^TX$的特征分解，$X^TX\\ =\\ VD^2V^T$,而且X的样本协方差矩阵$S=\\ X^TX/N$,所以显而易见(还是不太明白如何显而易见）：    \n",
    "&emsp; $Var(Xv_1) = \\frac{d^2_1}{N}$    \n",
    "通常对应较大特征值的特征向量方向有较大的方差，较小特征值的方向方差较小。    \n",
    "### 3.4.2 Lasso\n",
    "$ {\\hat \\beta}^{ridge} = \\arg \\min \\limits_p \\{\\sum \\limits^N_{i=1}(y_i - \\beta_0 - \\sum \\limits^p_{j=1} x_{ij}\\beta_j)^2\\ +\\ \\lambda\\sum\\limits^p_{j=1}|\\beta_j|\\}$    \n",
    "也可以写为：    \n",
    "$ {\\hat \\beta}^{ridge} = \\arg \\min \\limits_p \\sum \\limits^N_{i=1}(y_i - \\beta_0 - \\sum \\limits^p_{j=1} x_{ij}\\beta_j)^2$,     \n",
    "&emsp;&emsp; $subject\\ to\\  \\sum\\limits_{j=1}^p |\\beta_j| \\le t, $    \n",
    "$L_2$ ridge penalty $\\sum\\limits^p_{j=1}\\beta^2_j$ replaced by the $L_1$ lasso penalty $\\sum\\limits^p_{j=1}|\\beta_j| $    \n",
    "### 3.4.3 Discussion    \n",
    "**Ridge regression** do a propotional shrinkage, **lasso** translates each coefficient by a constant factor $\\lambda$, truncating at zero, called **\"soft threshholding\"**, **Best-subset selection** drops all variables with coefficients smaller than the Mth largest,called **\"hard threshholding\"**,如下图。    \n",
    "    <img src=\"./img/table3_4.png\" width=560 height=720>    \n",
    "此外，如下图所示，ridge regression contraint area like a disk, lasso contraint area like a rhomboid, so the elliptical contours is easy to first hit the **corner**（$\\beta_j$ equal to zero) of the lass constraint area espcially when p>2, the diamonds become a diamond which has more corners。       \n",
    "    <img src=\"./img/figure3_11.png\" width=560 height=720>     \n",
    "    \n",
    "### 3.4.4 Least Angle Regression\n",
    "1. 算法流程：\n",
    "    * first identifies the variable most correlated with the response\n",
    "    * move the coefficient of this variable continuously toward its least-squares value\n",
    "    * as soon as another varibable \"catch up\" in terms of correlation with the residual, join the second variable in the active set.\n",
    "    * then move together in a way that keeps theis correlation s tied and decreasing.    \n",
    "\n",
    "<img src=\"./img/Algorithm3_2.png\" width= 560 height=480>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
